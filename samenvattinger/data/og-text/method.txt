This section describes the method used to apply Q-learning to learn a robot to score a penalty. Firstly, the states and their representations that are used are explained in section 4.1, followed by the actions in section 4.2. Section 4.3 describes the different rewards that can be obtained from the environment and in section 4.4, the network which is used to learn and predict the Q-values is explained. Furthermore, an overview is given in section 4.5 of how these components are combined to create a learning process. Lastly, the evaluation method is described in section 4.6. The states describe the environment in which the robot is situated. The environment consists of various objects on different positions on the field, such as the ball, the goals, and other robots (both opponents and teammates). The states can be based on either local or global positions of objects in the field. We chose to use local positions (robot frame), since the positions of the objects are chiefly based on the observations of the robot made with its cameras and thus already relative to the robot. Moreover, the global positions are computed from the relative positions, so if a small error would occur in the relative position, a bigger error would occur in the global position. The position of the objects is discretised to reduce the state space. Each position is put into a bin based on its angle and its distance to the robot, see Figure 4. Eight bins for angles and five different distances can be used, which results in 40 bins per object. Depending on the experiment, all five distances or less are used. Bins that are closer to the robot are smaller than the bins farther away from the robot, as a higher precision is desired closer to the robot. In contrary to the actions in (Hester et al., 2010), that involve moving one joint, and (Smart and Kaelbling, 2002) which uses higher level behaviours, the actions that the robot can perform in this thesis are all single actions, such as walking in a specific direction or kicking. Figure 6 shows the thirteen actions the robot can perform. The robot learns from the rewards obtained from the environment. To let the robot learn as much as possible by itself, minimal amount of rewards are desirable. However, as the state space increases, many trials are needed to explore the states and reducing this amount of trials would be preferred. Since some actions lead to a higher probability of scoring, minimal guidance that encourages the robot to take these actions, can decrease the search space rapidly. Therefore, we experiment with different rewards. Initially, a reward is only given at the end of a trial. This minimal reward is comparable with a real match, as only the final score is taken into account. Scoring a goal results in a positive reward (2) and, when no goal is scored within a minute, a negative reward (-2) is given. However, a disadvantage of obtaining a reward only at the end of a match is that it results in sparse rewards, which can lead to many trails needed to learn a good policy, since many actions are needed to be taken before obtaining  any reward. Minimal guidance by giving intermediate rewards is expected to decrease the search space significantly. Since the ball is the most important object during a match, because it needs to end up in the goal of the opponent, minimal guidance uses the distance between the ball and the robot, and between the ball and the goal as intermediate rewards. Firstly, the distance between the robot and the ball is used as intermediate reward, as movement to the ball is expected to be a good action, since the robot has to move the ball towards the goal. If an action causes the robot to be closer to the ball compared to the state before, a positive reward (0.5) is given, otherwise a negative reward (-0.5) is given. Another intermediate reward is the distance between the ball and the goal, because an action that causes the ball to be closer to the goal is expected to be a good action. If an action causes the ball to be closer to the goal, a positive reward (1) is given, otherwise a negative reward (-1) is given.  To stimulate scoring using a kick, the reward for the distance between the ball and the goal is higher than the reward for the distance between the robot and the ball. If the robot scores by kicking, the distance to the ball increases, which results in a negative reward. Due to the bigger positive reward for the distance between the ball and the goal, the robot still receives a positive reward when scoring using a kick. Furthermore, the time left for the trial is given as additional reward when a goal is scored, to encourage the robot to score as fast as possible. The robot is expected to score more by kicking using this reward, since the time to score by kicking is lower than by walking the ball into the goal  The time is scaled to a number between 0 and 1 and added to the reward the robot received for scoring. Despite only the small amount of states used in this thesis, we use a neural network to learn and predict the Q-values instead of a lookup table, which would have used less memory as explained in 3.2. We chose to use a neural network to be able to easily expand this thesis to research that uses many states and actions  A five-layer fully-connected network is used to learn and predict the Qvalue  (Figure 3). The size of the input layer and the three hidden layers equals the size of the state vector. The amount of nodes in the output layer equals the amount of possible actions, which is thirteen. A rectified linear unit (ReLU) is used as activation function for the input layer and the three hidden layers. The output layer uses the identity function, since Q-values can be both positive and negative. Q-learning is implemented within the framework of the Dutch Nao Team 2016 (Lagrand et al., 2016), which is based on B-Human 2015  . The only major differences are the ball detector and the behaviour engine  This framework uses SimRobot (Laue and RÂ¨ofer, 2008) as simulation tool. SimRobot uses external XML files that are loaded at runtime to model the environment. These files can be easily edited to create the desired scenario, such as the one versus one penalty scene shown in Figure 7. The simulation is adjusted in order to have trials of 1 minute and a goal for the opponent team if no goal is scored within this minute. A simulation is used to circumvent the limitation a real robot has with the amount of trials. Moreover, a fully observable field is used in simulation, to ensure that the focus can be on learning behaviours, without having errors in the position of the objects due to poor observations of the robot. A session is one training session in which a robot learns to score by repeatedly starting an attempt to score, a trial. Figure 8 shows an overview of the training process, the outer (red) circle represents a trial while the inner (blue) circle is executed every time the robot enters a different state  When an action would be chosen every simulation cycle, different actions would be executed while the robot is still in the same state, due to exploration  The previously chosen action would only have had one simulation step to execute itself, while they require more than one simulation cycle to have some effect on the environment and to get a relevant reward  By starting a session the network is initialised (1) with random weights and the first trial is started. At the start of each trial the robots and the ball are placed on their starting positions, the time is set to one minute and a new attempt begins (2). The attacking team now tries to score within a minute by repeatedly obtaining the state it is in (3), selecting and performing an action (4), obtaining the reward (5) and updating the network (6). The state information is obtained from the environment by computing the state bin (section 4.1) of which a state vector is created  Subsequently, the state vector is used as input for the network to predict the Q-values of the actions. Boltzmann action selection (3.3) is now applied on the predicted Q-values to select the next action and by performing this action, the next state and the reward are obtained from the environment. Next, the Q-value is updated and the network is trained with the state vector as input and the updated Q-values as output. These steps are repeated until a trial ends, which is after one minute or after a goal is scored. If the robot has scored x times in a row, the session ends  otherwise, a new trial is started. The robot has learnt a good action policy and stops training when it has scored x times in a row. x differs for the different experiments, since scoring by kicking is less reliable than walking the ball into the goal. In the experiments in which a goal can be easily scored by walking, x is higher  to ensure the robot has explored enough other actions and states before it stops training. Lower values of x are used in situations where the robot has to score by kicking, because one missed attempt quickly leads to going to explore many other actions before trying the action that previously led to a goal again. Since the network is always randomly initialised and high variation in the amount of trials can occur between different sessions, each experiment is performed multiple times. To evaluate the speed at which a network learns to score, the average amount of trials needed to train the network is computed. The performance of the learnt network is evaluated by computing the accuracy over fifty attempts to score, by averaging over the different sessions.