Playing football with robots is more than programming how robots should walk and shoot, the (team)tactics of the robots play a major part during a match, which leads to the question what a robot should do in a specific situation. For example, while standing in front of the ball and the goal, and no keeper in between them, the robot should kick the ball and score. Driven by the popularity of the RoboCup Standard Platform League (SPL) competition1, where all robots are identical in hardware, we focus on the Nao robot2 shown in Figure 1. Currently, most behaviours of the robots are based on rules that the teams invent themselves, such as shooting the ball if the robot is at a certain distance to the ball (Lagrand et al., 2016) (RÂ¨ofer et al., 2016). However, these man-made rules require a large amount of precision and many small adjustments of parameters. For example, the parameter that defines the distance to the ball before kicking differs for various fields and needs to be manually adjusted until the correct value is obtained. Therefore the main hypothesis of this thesis is that tactic should be learnt from experience and not programmed by engineers. Reinforcement Learning uses rewards obtained from the environment to learn actions that can be taken, resulting in an action policy that is learnt from own experiences. For example, Reinforcement Learning has been used for learning to play games like Pong (Mnih et al., 2015), where only the pixels of a game state were used as input environment and the obtained score as reward. Depending on the scenario, rewards are only given at the end of the game (e.g. chess or checkers), or already during the game (e.g. Pong, SuperMario). The action policy is learnt from its own experience and used to play these games successfully. Since Reinforcement Learning learns from experiences, we experiment with it, using minimal reward set-ups to learn a robot to score a penalty. Contrary to preceding research, we focus on learning the actions that lead to a scoring behaviour and not the joint values (Hester et al., 2010) or higher level behaviours (Smart and Kaelbling, 2002). Different scenarios for learning behaviours will be validated to demonstrate that behaviours can be learnt using Reinforcement Learning. The next section discusses preceding research about reinforcement learning. In section 3, Q-learning is explained. Section 4 describes the method used to learn a robot its own behaviour and section 5 presents the experiments that were performed together with their results. Finally, section 6 presents the conclusion and discussion, and future work is proposed.