The experiments show that a robot is able to learn how to score a penalty using Reinforcement Learning. With no goalkeeper defending the goal, walking the ball into the goal is the most successful behaviour for scoring. This can be explained by the fact that kicking is less reliable, because the ball can be hit at different positions, resulting in the ball moving in a slightly different direction and missing the goal. Having a goalkeeper that defends the goal results in a behaviour that scores by kicking, since the path taken by walking straight ahead with the ball is now obstructed by the goalkeeper, while needing on average 84% more trials to optimize. The search space and thus the amount of trials can be reduced in different manners, without effecting the accuracy of the learnt policy. Firstly, intermediate rewards used to guide the learning by taking the distance between the ball and the robot, and between the ball and the goal into account, can reduce the amount of trials up to 75%. Furthermore, combining states additively instead of by multiplying to decrease the size of the state vector reduces the amount of trials with 58%. Currently, the reward for scoring a goal and missing a goal have the same value with the difference being that when scoring the reward is applied positively (2) and when missed negatively (-2). However, most attempts do not lead to a goal, thus a negative reward is given more often, causing the positive reward to have relatively small impact on the Q-values. Future research can experiment with different rewards to increase the impact of positive rewards on the Q-values. Furthermore, objects in states far away from the robot are often in the same bin. For example, the robot and the goal are in the same bin at the start of a trial (Figure 4b). However, having the goal and the goalkeeper in different bins seems to be more useful, since the goalkeeper is defending the goal. If the robot knows the difference between the positions of the goal and the goalkeeper, the robot can learn to kick past the goalkeeper into the goal. Future research can experiment with different state bins, for example, by using non equally divided bins, such as more (and smaller) bins in front of the robot and less (and bigger) bins behind the robot as shown in Figure 17. Besides, the state space of the robot can be reduced to only that part the robot can observe (Figure 18), to ensure that it is more realistic to a real robot, and simpler to extend to a real robot. When extending to a real robot, a network that is trained with simulation can be used as initial network for training on a robot to optimise it further for use on the real robot, also known as transfer learning (Taylor and Stone, 2009). This thesis presents a foundation of learning the actions that form a tactic. The next step is to extend this research to more complex scenarios, such as a moving goalkeeper, or a goalkeeper that learns its behaviour concurrently. Another complex scenario is a two-versus-one situation, in which a teammate is added to be able to learn team tactics.