Q-learning (Watkins, 1989) is a Reinforcement Learning algorithm that learns an action-value function (Q) which represents the expected utility (Q-value) of taking a given action and state. The optimal policy that follows is constructed by selecting the action with the highest Q-value. An agent starts with randomly initialised Q-values and obtains the state (s) from the environment. Using action selection (section 3.3), an action (a) is selected that will be performed in this state. After performing the action, the reward (r) for taking the action in this state, and the next state (s 0 ) are obtained. The classic Q-learning algorithm makes use of a lookup table to save the Q-values of all state-action pairs. The table consists of nstates rows and nactions columns (Figure 2). Each time a Q-value is updated (1), the Qvalues for this state and the next states are obtained from the table and used to update the Q-value. One of the advantages of a lookup table is that the exact Q-values are available for each state-action pair. Therefore, the action policy found will be highly accurate. Saving all Q-values is also a disadvantage, as it needs a large amount of memory, when the amount of states and actions increases. Another method, which is useful for handling a large amount of states and actions, is Q-learning combined with a neural network. Since neural networks can use multiple hidden layers that are smaller than the amount of states and actions, the memory required is lower than that for a lookup table in cases with a large amount of states and actions. However, when using a small amount of states and actions, a lookup table uses less memory. Instead of saving the Q-value of each state-action pair, a neural network, having a vector representing the state as input and the Q-values of each action as output, is used (Figure 3). Each time a Q-value is updated (1), the network is trained having the state vector as input and the updated Q-value as output. The disadvantage of a neural network is that it only estimates the Q-values, which results in noisy updates and thus an even more noisy neural network. Action selecting is used to determine which action to take based on the acquired Q-values. Several methods are introduced (Sutton and Barto, 1998, p. 30-31), such as the greedy method, ε-greedy and softmax action selection. The simplest action selection is to always select the action with the highest Q-value. This greedy method always exploits current knowledge, but never explores other (possibly better) actions. To be able to explore other actions, ε-greedy is a good alternative. εgreedy is mostly greedy, but with a probability of ε, an action is randomly selected amongst all actions. An advantage of ε-greedy is the certainty that every action will be sampled when the amount of trials increases. However, the disadvantage is that it selects equally among all actions when exploring, which results in choosing the worse action with the same likelihood as the second best action. Softmax action selection avoids this equally distributed selection by computing a weighted distribution based on the Q-values. Boltzmann action selection is one of the most common softmax methods.