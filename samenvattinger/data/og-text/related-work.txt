Presently, Reinforcement Learning is used for many different purposes. For example, a Reinforcement Learning algorithm to automatically learn a controller for helicopters is applied by Ng et al. (2006). In Mnih et al. (2015), Reinforcement Learning was applied to learn a computer to play classic Atari 2600 games. Using a deep neural network, a winning strategy for different games was learnt based on the most raw data available namely the pixels of the image rendered by the game and the game scores. In robotics, Reinforcement Learning is used less often, primarily since the considerable amount of trials. By using a simulator for this research, we circumvent this problem and can run a large number of trials. Contrary to the behaviours learnt in this thesis, most Reinforcement Learning that is applied in robotics concerns control policies. For example, in Smart and Kaelbling (2002) Reinforcement Learning was applied to learn control policies for mobile robots. Q-learning (Watkins, 1989) was used in order to learn a robot two tasks, which are Corridor Following and Obstacle Avoidance. During both tasks, the rotation speed of the robot was successfully learnt in approximately two hours and the robot was able to complete the tasks. Q-learning requires discrete states and actions, which is often not the case with mobile robots. To overcome this problem, Qlearning was implemented with HEDGER (Smart and Kaelbling, 2000), an algorithm that safely approximates the value function for continuous state control tasks. Furthermore, the use of Reinforcement Learning in order that a robot learns to kick a penalty is described in (Hester et al., 2010). A robot learnt how many degrees it had to shift its leg outwards, before kicking the ball. Reinforcement Learning with decision trees (Pyeatt, 2003) was applied in order to limit the number of trials needed for learning. This algorithm uses decision trees instead of a lookup table or a neural network to acquire the best policy for enormous problems. A control policy for shifting the leg outwards was learnt and the robot was able to score on 85% of its attempts. While Hester et al. are learning the joint positions for scoring a penalty, we focus on higher level actions that together form a scoring tactic. When using robots in simulation, not only control policies, but also behaviour policies are learnt. Stone et al. (2005) addresses a three versus two keepaway football problem in simulation. In this situation, a team of three robots, the keepers, should maintain possession of the ball, while two takers try to capture the ball. A semi-Markov decision process (SMDP) (Baykal-G¨ursoy and G¨ursoy, 2007) is used to let the robots learn their actions, such as Receive, Pass or Hold. Stone et al. show that the policy learnt could hold the ball an average of 9.6 seconds while a hand-coded policy was able to hold the ball for an average of 8.2 seconds. In contrary to the high level behaviours learnt by them, this thesis focusses on learning the actions that together form a tactic, in order that no tactics, such as hold the ball, have to be predefined as they do.