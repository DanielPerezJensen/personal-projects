Since neural networks can use multiple hidden layers that are smaller than the amount of states and actions, the memory required is lower than that for a lookup table in cases with a large amount of states and actions. Instead of saving the Q-value of each state-action pair, a neural network, having a vector representing the state as input and the Q-values of each action as output, is used (Figure 3). Each time a Q-value is updated (1), the network is trained having the state vector as input and the updated Q-value as output. Action selecting is used to determine which action to take based on the acquired Q-values. The simplest action selection is to always select the action with the highest Q-value.