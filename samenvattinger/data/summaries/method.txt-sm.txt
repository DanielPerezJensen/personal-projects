Firstly, the states and their representations that are used are explained in section 4.1, followed by the actions in section 4.2. Section 4.3 describes the different rewards that can be obtained from the environment and in section 4.4, the network which is used to learn and predict the Q-values is explained. We chose to use local positions (robot frame), since the positions of the objects are chiefly based on the observations of the robot made with its cameras and thus already relative to the robot. Scoring a goal results in a positive reward (2) and, when no goal is scored within a minute, a negative reward (-2) is given. Since the ball is the most important object during a match, because it needs to end up in the goal of the opponent, minimal guidance uses the distance between the ball and the robot, and between the ball and the goal as intermediate rewards. To stimulate scoring using a kick, the reward for the distance between the ball and the goal is higher than the reward for the distance between the robot and the ball. Due to the bigger positive reward for the distance between the ball and the goal, the robot still receives a positive reward when scoring using a kick. A session is one training session in which a robot learns to score by repeatedly starting an attempt to score, a trial. The state information is obtained from the environment by computing the state bin (section 4.1) of which a state vector is created  Subsequently, the state vector is used as input for the network to predict the Q-values of the actions. x differs for the different experiments, since scoring by kicking is less reliable than walking the ball into the goal.